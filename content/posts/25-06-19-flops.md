---
title: "short note on FLOPs"
date: 2025-06-19
series: ["PaperMod"]
weight: 1
aliases: ["/papermod-installation"]
tags: ["flops", "inference", "training"]
author: ["Tanvi Bhandarkar"]
cover:
  image: images/papermod-cover.png
  hiddenInList: true
---

- basic unit of compute
  - GPT-3 (175B) â‰ˆ 350 GFLOPs per token
  - multiplying two floating point numbers = 1 FLOP
  - measures raw processing power of hardware by performing calculations (operations) in one second
- operations on floating-point numbers (numbers with decimal points):
  - matrix multiplications
  - attention computations
  - activation functions
- FLOP Utilization Rate or Model FLOPs Utilization (MFU)
  - % at which the hardware's (CPU, GPU, TPU or any accelerator) max capacity is being used
  - ex, GPU peak capacity is 100 TFLOP/s (theoretical peak FLOPS) under ideal conditions, LLM inference only uses 40 (achieved FLOPS) => FLOP utilization rate is 40%
    - `FLOP Utilization Rate = (Achieved / Theoretical Peak FLOPS) * 100`
  <details>
    <summary>how is "achieved" FLOPs potentially measured?</summary>
    This content is hidden until the user clicks the toggle.
  </details>